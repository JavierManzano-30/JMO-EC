# üìñ Instrucciones de Uso - BubblyBot

## üöÄ C√≥mo ejecutar el chatbot

### Paso 1: Preparar LM Studio
1. Abre **LM Studio** en tu ordenador
2. Carga un modelo de lenguaje (cualquier modelo compatible)
3. Activa el **servidor local** en LM Studio:
   - Ve a la secci√≥n "Local Server" o "Server"
   - Configura el puerto: **1234**
   - Inicia el servidor
   - Aseg√∫rate de que est√© escuchando en `http://127.0.0.1:1234`

### Paso 2: Instalar dependencias del proyecto (solo la primera vez)
Abre una terminal en la carpeta del proyecto (`T1\SPRINT4`) y ejecuta:
```bash
npm install
```

### Paso 3: Ejecutar el chatbot
En la misma terminal, ejecuta:
```bash
npm run dev
```

Esto iniciar√° el servidor de desarrollo de Vite. Ver√°s algo como:
```
  VITE v7.x.x  ready in xxx ms

  ‚ûú  Local:   http://localhost:5173/
  ‚ûú  Network: use --host to expose
```

### Paso 4: Abrir en el navegador
1. Abre tu navegador (Chrome, Firefox, Edge, etc.)
2. Ve a la direcci√≥n que aparece en la terminal, normalmente:
   ```
   http://localhost:5173
   ```

### Paso 5: ¬°Usar el chatbot!
1. Escribe tu mensaje en el cuadro de texto
2. Presiona Enter o haz clic en el bot√≥n de enviar
3. El chatbot responder√° usando el modelo de LM Studio

---

## ‚öôÔ∏è Configuraci√≥n del servidor LM Studio

### Configuraci√≥n recomendada:
- **Puerto**: 1234
- **Host**: 127.0.0.1 (localhost)
- **API compatible**: OpenAI

### Verificar que el servidor est√° funcionando:
Abre en tu navegador: `http://127.0.0.1:1234/v1/models`

Deber√≠as ver un JSON con la lista de modelos disponibles.

---

## üõ†Ô∏è Comandos √∫tiles

### Ejecutar en modo desarrollo:
```bash
npm run dev
```

### Crear versi√≥n para producci√≥n:
```bash
npm run build
```

### Verificar c√≥digo (linter):
```bash
npm run lint
```

### Previsualizar versi√≥n de producci√≥n:
```bash
npm run build
npm run preview
```

---

## ‚ùì Soluci√≥n de problemas

### Error: "No se pudo conectar con el modelo"
- Verifica que LM Studio est√© ejecut√°ndose
- Verifica que el servidor est√© activo en el puerto 1234
- Aseg√∫rate de que hay un modelo cargado en LM Studio

### Error: "npm no se reconoce"
- Aseg√∫rate de tener Node.js instalado
- Descarga Node.js desde: https://nodejs.org/

### El chatbot no responde
- Revisa la consola del navegador (F12)
- Verifica que el servidor de LM Studio est√© respondiendo
- Intenta reiniciar LM Studio y el servidor

### El puerto 1234 est√° ocupado
- Cambia el puerto en LM Studio a otro (ej: 1235)
- Actualiza la URL en `src/services/lmstudio.js`:
  ```javascript
  const LM_STUDIO_BASE_URL = 'http://127.0.0.1:1235';
  ```

---

## üìù Notas importantes

- El chatbot mantiene el contexto de la conversaci√≥n (√∫ltimos 10 mensajes)
- Las respuestas dependen del modelo que tengas cargado en LM Studio
- El procesamiento es local, no se env√≠a informaci√≥n a internet
- Puedes usar cualquier modelo compatible con OpenAI API

---

## üéØ Resumen r√°pido

1. **Abre LM Studio** ‚Üí Carga modelo ‚Üí Activa servidor en puerto 1234
2. **Abre terminal** en la carpeta del proyecto
3. **Ejecuta**: `npm install` (solo primera vez)
4. **Ejecuta**: `npm run dev`
5. **Abre navegador**: http://localhost:5173
6. **¬°Chatea!** üí¨

